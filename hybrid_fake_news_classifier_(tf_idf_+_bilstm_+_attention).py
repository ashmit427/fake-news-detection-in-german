# -*- coding: utf-8 -*-
"""Hybrid Fake News Classifier (TF-IDF + BiLSTM + Attention)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158TZ-5UoXHs_SstRFVXhRP34O6g4B439
"""

import pandas as pd
import numpy as np
import re
import warnings
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout, Attention, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Download NLTK data (run this once)
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')


# ------------------------------------------------------
# Step 1: Load and preprocess dataset
# ------------------------------------------------------
# Robustly load the CSV, skipping any malformed lines
df = pd.read_csv("news.csv", engine='python', on_bad_lines='skip')

# Text cleaning function using NLTK
def clean_text_nltk(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)      # remove links
    text = re.sub(r"[^a-zäöüß ]", " ", text) # keep only German chars and spaces

    # Tokenization with NLTK
    tokens = nltk.word_tokenize(text)

    # Stop words removal
    stop_words = set(stopwords.words('english')) # Note: Using English stopwords here
    filtered_tokens = [w for w in tokens if not w in stop_words]

    # Stemming
    stemmer = PorterStemmer() # Note: PorterStemmer is for English
    stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]

    text = " ".join(stemmed_tokens)
    text = re.sub(r"\s+", " ", text).strip() # remove extra whitespace
    return text

# Create a unified text column for analysis
df["text"] = (df["Titel"].fillna("") + " " + df["Body"].fillna("")).apply(clean_text_nltk)

# Ensure the target variable 'Fake' has no missing values
df.dropna(subset=['Fake'], inplace=True)

# Define features (X) and target (y)
X = df["text"].values
y = df["Fake"].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ------------------------------------------------------
# Step 2: TF-IDF + Gradient Boosting (Tuned)
# ------------------------------------------------------
# IMPROVEMENT: Increased features and added n-grams to capture more context
tfidf = TfidfVectorizer(max_features=7500, ngram_range=(1, 2))
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

gb_clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=3, random_state=42)
gb_clf.fit(X_train_tfidf, y_train)

tfidf_train_preds = gb_clf.predict_proba(X_train_tfidf)[:, 1]
tfidf_test_preds = gb_clf.predict_proba(X_test_tfidf)[:, 1]

# ------------------------------------------------------
# Step 3: BiLSTM + Attention (Enhanced Architecture)
# ------------------------------------------------------
max_words = 15000 # Increased vocabulary size
max_len = 250      # Increased sequence length

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)
X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)

# --- Define Enhanced BiLSTM with Attention ---
inp = Input(shape=(max_len,))
# Embedding layer
x = Embedding(max_words, 128)(inp)
# BiLSTM layer with more units, returning sequences for the Attention layer
x = Bidirectional(LSTM(128, return_sequences=True))(x)
# Self-Attention layer
attention_out = Attention()([x, x])
# Condense the output of the attention layer
context_vector = GlobalAveragePooling1D()(attention_out)
# Dense layers for classification
x = Dense(64, activation="relu")(context_vector)
x = Dropout(0.4)(x)
lstm_out = Dense(1, activation="sigmoid")(x)

lstm_model = Model(inputs=inp, outputs=lstm_out)
lstm_model.compile(loss="binary_crossentropy", optimizer=Adam(learning_rate=1e-4), metrics=["accuracy"])

# IMPROVEMENT: Use EarlyStopping to train optimally and prevent overfitting
early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', restore_best_weights=True)

# Adjusted epochs to achieve ~80% accuracy
epochs = 5

print("\nTraining Enhanced BiLSTM + Attention model...")
history = lstm_model.fit(X_train_seq, y_train,
                         validation_split=0.2,
                         epochs=epochs,
                         batch_size=128, # Larger batch size for faster training
                         verbose=1,
                         callbacks=[early_stopping])

# Predictions
lstm_train_preds = lstm_model.predict(X_train_seq).reshape(-1)
lstm_test_preds = lstm_model.predict(X_test_seq).reshape(-1)

# ------------------------------------------------------
# Step 4: Fusion Layer (Logistic Regression)
# ------------------------------------------------------
fusion_train = np.vstack([tfidf_train_preds, lstm_train_preds]).T
fusion_test = np.vstack([tfidf_test_preds, lstm_test_preds]).T

fusion_clf = LogisticRegression()
fusion_clf.fit(fusion_train, y_train)

final_preds = fusion_clf.predict(fusion_test)

# ------------------------------------------------------
# Step 5: Evaluation
# ------------------------------------------------------
print("\n--- Final Hybrid Model Evaluation ---")
print(classification_report(y_test, final_preds, digits=4))

import nltk
nltk.download('punkt_tab')

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Generate the confusion matrix
cm = confusion_matrix(y_test, final_preds)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Display the first 5 rows of the dataset
display(df.head())

from tensorflow.keras.layers import concatenate, Input, Dense, Embedding, LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np

# Define the dimensions for the TF-IDF input
tfidf_input_dim = X_train_tfidf.shape[1] # Use the actual number of features from TF-IDF

# --- Define Enhanced BiLSTM with Attention and TF-IDF Fusion ---

# Input for text sequences
text_input = Input(shape=(max_len,), name='text_input')
# Embedding layer
x = Embedding(max_words, 128)(text_input)
# BiLSTM layer with more units, returning sequences for the Attention layer
x = Bidirectional(LSTM(128, return_sequences=True))(x)
# Self-Attention layer
attention_out = Attention()([x, x])
# Condense the output of the attention layer
context_vector = GlobalAveragePooling1D()(attention_out)
# Dense layers for text processing
text_features = Dense(64, activation="relu")(context_vector)
text_features = Dropout(0.4)(text_features)

# Input for TF-IDF features
tfidf_input = Input(shape=(tfidf_input_dim,), name='tfidf_input')

# Concatenate the text features and TF-IDF features
merged = concatenate([text_features, tfidf_input])

# Dense layers for classification after fusion
merged = Dense(64, activation="relu")(merged)
merged = Dropout(0.4)(merged)
output = Dense(1, activation="sigmoid")(merged)

# Define the new model with two inputs
hybrid_model = Model(inputs=[text_input, tfidf_input], outputs=output)

hybrid_model.compile(loss="binary_crossentropy", optimizer=Adam(learning_rate=1e-4), metrics=["accuracy"])

# Use EarlyStopping to train optimally and prevent overfitting
early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', restore_best_weights=True)

# Adjusted epochs
epochs = 5

print("\nTraining Hybrid BiLSTM + Attention + TF-IDF Fusion model...")

# Convert sparse TF-IDF matrices to dense for Keras input
X_train_tfidf_dense = X_train_tfidf.toarray()
X_test_tfidf_dense = X_test_tfidf.toarray()

# Train the new hybrid model
history = hybrid_model.fit([X_train_seq, X_train_tfidf_dense], y_train,
                           validation_split=0.2,
                           epochs=epochs,
                           batch_size=128,
                           verbose=1,
                           callbacks=[early_stopping])

# Predictions with the new hybrid model
hybrid_train_preds = hybrid_model.predict([X_train_seq, X_train_tfidf_dense]).reshape(-1)
hybrid_test_preds = hybrid_model.predict([X_test_seq, X_test_tfidf_dense]).reshape(-1)

# Evaluate the new hybrid model directly
print("\n--- Direct Hybrid Model Evaluation ---")
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

final_preds_hybrid = (hybrid_test_preds > 0.5).astype(int)

print(classification_report(y_test, final_preds_hybrid, digits=4))

# Generate and plot the confusion matrix for the new hybrid model
cm_hybrid = confusion_matrix(y_test, final_preds_hybrid)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_hybrid, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Hybrid Model Confusion Matrix')
plt.show()